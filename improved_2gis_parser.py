import asyncio
import logging
import random
import re
from datetime import datetime
from typing import Dict, List, Optional
import pandas as pd
from playwright.async_api import async_playwright, Page, Browser
import argparse
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('2gis_parser.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class GISParser:
    def __init__(self, city: str = "–ê—Å—Ç–∞–Ω–∞", max_items_per_category: int = 100):
        self.city = city
        self.max_items_per_category = max_items_per_category
        self.results = []
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π
        self.processed_companies = set()  # –î–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        self.company_details = {}  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        
    def normalize_company_name(self, name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not name or name == '–ù–µ —É–∫–∞–∑–∞–Ω–æ':
            return ''
            
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        normalized = name.lower().strip()
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã
        normalized = re.sub(r'\s+', ' ', normalized)
        normalized = re.sub(r'[^\w\s]', '', normalized, flags=re.UNICODE)
        
        # –£–±–∏—Ä–∞–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã —Ñ–∏–ª–∏–∞–ª–æ–≤
        suffixes_to_remove = [
            r'\s*—Ñ–∏–ª–∏–∞–ª\s*\d*',
            r'\s*–æ—Ç–¥–µ–ª–µ–Ω–∏–µ\s*\d*', 
            r'\s*–º–∞–≥–∞–∑–∏–Ω\s*\d*',
            r'\s*—Ç–æ—á–∫–∞\s*\d*',
            r'\s*‚Ññ\s*\d+',
            r'\s*\d+\s*$',  # –¶–∏—Ñ—Ä—ã –≤ –∫–æ–Ω—Ü–µ
            r'\s*mall\s*',
            r'\s*—Ç—Ü\s*',
            r'\s*—Ç–¥—Ü\s*',
            r'\s*—Ü–µ–Ω—Ç—Ä\s*',
            r'\s*—Ñ—É–¥–∫–æ—Ä—Ç\s*',
            r'\s*—Ç–æ—Ä–≥–æ–≤—ã–π\s*—Ü–µ–Ω—Ç—Ä\s*',
        ]
        
        for suffix in suffixes_to_remove:
            normalized = re.sub(suffix, '', normalized, flags=re.IGNORECASE)
        
        # –£–±–∏—Ä–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–Ω—Ç—Ä–æ–≤ –≤ —Å–∫–æ–±–∫–∞—Ö –∏–ª–∏ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
        normalized = re.sub(r'\([^)]*\)', '', normalized)  # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –≤ —Å–∫–æ–±–∫–∞—Ö
        normalized = re.sub(r',.*$', '', normalized)  # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        normalized = normalized.strip()
        
        return normalized
    
    def is_company_already_processed(self, name: str, address: str = None) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –±—ã–ª–∞ –ª–∏ –∫–æ–º–ø–∞–Ω–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞"""
        normalized_name = self.normalize_company_name(name)
        
        if not normalized_name:
            return False
            
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        if normalized_name in self.processed_companies:
            logger.info(f"üîÑ –ö–æ–º–ø–∞–Ω–∏—è '{name}' —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ (–¥—É–±–ª–∏–∫–∞—Ç)")
            return True
            
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –ø–æ—Ö–æ–∂–∏–º –Ω–∞–∑–≤–∞–Ω–∏—è–º
        for existing_name in self.processed_companies:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –æ–¥–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –¥—Ä—É–≥–æ–º)
            if len(normalized_name) > 3 and len(existing_name) > 3:
                if normalized_name in existing_name or existing_name in normalized_name:
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∞–¥—Ä–µ—Å –µ—Å–ª–∏ –µ—Å—Ç—å
                    if address and existing_name in self.company_details:
                        existing_address = self.company_details[existing_name].get('address', '')
                        if existing_address and address:
                            # –ï—Å–ª–∏ –∞–¥—Ä–µ—Å–∞ —Ä–∞–∑–Ω—ã–µ, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ —Ä–∞–∑–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
                            if not self.addresses_similar(address, existing_address):
                                continue
                    
                    logger.info(f"üîÑ –ù–∞–π–¥–µ–Ω–∞ –ø–æ—Ö–æ–∂–∞—è –∫–æ–º–ø–∞–Ω–∏—è: '{name}' ‚âà '{existing_name}' (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)")
                    return True
        
        return False
    
    def addresses_similar(self, addr1: str, addr2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∞–¥—Ä–µ—Å–æ–≤"""
        if not addr1 or not addr2:
            return False
            
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∞–¥—Ä–µ—Å–∞
        addr1_norm = re.sub(r'[^\w\s]', ' ', addr1.lower())
        addr2_norm = re.sub(r'[^\w\s]', ' ', addr2.lower())
        
        # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ –¥–æ–º–æ–≤/–∫–≤–∞—Ä—Ç–∏—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞–π–æ–Ω–∞
        addr1_base = re.sub(r'\d+', '', addr1_norm).strip()
        addr2_base = re.sub(r'\d+', '', addr2_norm).strip()
        
        # –ï—Å–ª–∏ –±–∞–∑–æ–≤—ã–µ —á–∞—Å—Ç–∏ –∞–¥—Ä–µ—Å–æ–≤ —Å–æ–≤–ø–∞–¥–∞—é—Ç –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 70%
        common_words = set(addr1_base.split()) & set(addr2_base.split())
        total_words = set(addr1_base.split()) | set(addr2_base.split())
        
        if total_words and len(common_words) / len(total_words) > 0.7:
            return True
            
        return False
    
    def add_company_to_processed(self, name: str, business_info: Dict):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –≤ —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö"""
        normalized_name = self.normalize_company_name(name)
        if normalized_name:
            self.processed_companies.add(normalized_name)
            self.company_details[normalized_name] = {
                'original_name': name,
                'address': business_info.get('–ê–¥—Ä–µ—Å', ''),
                'category': business_info.get('–ö–∞—Ç–µ–≥–æ—Ä–∏—è', ''),
                'phone': business_info.get('–¢–µ–ª–µ—Ñ–æ–Ω', ''),
                'website': business_info.get('–°–∞–π—Ç', '')
            }
            logger.debug(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ: '{normalized_name}'")
        
    async def setup_browser(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∑–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞"""
        self.playwright = await async_playwright().start()
        
        self.browser = await self.playwright.chromium.launch(
            headless=False,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-features=VizDisplayCompositor',
                '--ignore-certificate-errors',
                '--ignore-ssl-errors',
                '--ignore-certificate-errors-spki-list'
            ]
        )
        
        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 920},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            java_script_enabled=True,
            ignore_https_errors=True
        )
        
        self.page = await context.new_page()
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç—ã
        self.page.set_default_timeout(30000)
        self.page.set_default_navigation_timeout(30000)
        
        # –ë–ª–æ–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç—è–∂–µ–ª—ã–µ —Ä–µ—Å—É—Ä—Å—ã
        await self.page.route("**/*.{png,jpg,jpeg,gif,webp,svg,mp4,avi,mov}", lambda route: route.abort())
        
        logger.info("–ë—Ä–∞—É–∑–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω")
        
    async def random_delay(self, min_sec: float = 1.0, max_sec: float = 3.0):
        """–°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞"""
        delay = random.uniform(min_sec, max_sec)
        logger.debug(f"–ó–∞–¥–µ—Ä–∂–∫–∞ {delay:.1f} —Å–µ–∫—É–Ω–¥")
        await asyncio.sleep(delay)
        
    async def open_2gis_and_search(self, category: str):
        """–û—Ç–∫—Ä—ã—Ç–∏–µ 2–ì–ò–° –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã URL
            urls_to_try = [
                f"https://2gis.kz/{self.city.lower()}",
                f"https://2gis.kz/astana",
                "https://2gis.kz"
            ]
            
            page_loaded = False
            for url in urls_to_try:
                try:
                    logger.info(f"–ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å: {url}")
                    await self.page.goto(url, wait_until="domcontentloaded", timeout=20000)
                    await self.random_delay(2, 4)
                    page_loaded = True
                    logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
                    break
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}: {e}")
                    continue
            
            if not page_loaded:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É 2–ì–ò–°")
                return False
            
            # –ò—â–µ–º –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏
            search_selectors = [
                'input[placeholder*="–ü–æ–∏—Å–∫"]',
                'input[placeholder*="–ø–æ–∏—Å–∫"]',
                'input[type="search"]',
                'input[name="search"]',
                '.search-input input',
                'input'
            ]
            
            search_input = None
            for selector in search_selectors:
                try:
                    search_input = await self.page.wait_for_selector(selector, timeout=5000)
                    if search_input:
                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞: {selector}")
                        break
                except:
                    continue
            
            if not search_input:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞")
                return False
            
            # –í–≤–æ–¥–∏–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            await search_input.fill('')
            await self.random_delay(0.5, 1)
            await search_input.type(category, delay=100)
            await self.random_delay(1, 2)
            
            # –ù–∞–∂–∏–º–∞–µ–º Enter
            await self.page.keyboard.press('Enter')
            await self.random_delay(5, 7)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è
            current_url = self.page.url
            if category.lower() in current_url.lower() or 'search' in current_url.lower():
                logger.info(f"–ü–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ: {category}")
                return True
            else:
                logger.warning(f"–ü–æ–∏—Å–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω. –¢–µ–∫—É—â–∏–π URL: {current_url}")
                # –î–∞–µ–º –µ—â–µ –æ–¥–Ω—É –ø–æ–ø—ã—Ç–∫—É
                await self.random_delay(3, 5)
                return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return False
            
    async def get_business_links_pagination_fixed(self):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ø–∞–≥–∏–Ω–∞—Ü–∏—è - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            await self.random_delay(3, 5)
            
            logger.info("üîç –ó–∞–ø—É—Å–∫ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –ø–∞–≥–∏–Ω–∞—Ü–∏–∏...")
            
            all_unique_links = set()
            current_page = 1
            max_pages = 50  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç
            consecutive_failures = 0
            max_failures = 3  # –ï—Å–ª–∏ 3 —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥ –Ω–µ –º–æ–∂–µ–º –ø–µ—Ä–µ–π—Ç–∏ - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
            
            while current_page <= max_pages and consecutive_failures < max_failures:
                logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {current_page}")
                
                # –°–∫—Ä–æ–ª–ª–∏–º –≤–≤–µ—Ä—Ö –ø–µ—Ä–µ–¥ —Å–±–æ—Ä–æ–º —Å—Å—ã–ª–æ–∫
                await self.page.evaluate("window.scrollTo(0, 0)")
                await self.random_delay(2, 3)
                
                # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                current_links = await self.collect_links_from_current_page()
                old_count = len(all_unique_links)
                all_unique_links.update(current_links)
                new_count = len(all_unique_links)
                new_links = new_count - old_count
                
                logger.info(f"üìä –°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page}: –¥–æ–±–∞–≤–ª–µ–Ω–æ {new_links} –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ (–≤—Å–µ–≥–æ: {new_count})")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
                if new_count >= self.max_items_per_category:
                    logger.info(f"üéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç {self.max_items_per_category} —Å—Å—ã–ª–æ–∫!")
                    break
                
                # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫, –Ω–æ —ç—Ç–æ –Ω–µ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –≤–æ–∑–º–æ–∂–Ω–æ –∫–æ–Ω–µ—Ü
                if new_links == 0 and current_page > 1:
                    logger.info("‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ - –≤–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
                    break
                
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                next_page_found = await self.go_to_next_page_fixed(current_page + 1)
                
                if next_page_found:
                    consecutive_failures = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –Ω–µ—É–¥–∞—á
                    current_page += 1
                else:
                    consecutive_failures += 1
                    logger.info(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {current_page + 1} (–ø–æ–ø—ã—Ç–∫–∞ {consecutive_failures}/{max_failures})")
                    
                    if consecutive_failures >= max_failures:
                        logger.info("üèÅ –ë–æ–ª—å—à–µ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
                        break
            
            business_urls = list(all_unique_links)[:self.max_items_per_category]
            logger.info(f"üéâ –ò–¢–û–ì–û —Å–æ–±—Ä–∞–Ω–æ {len(business_urls)} —Å—Å—ã–ª–æ–∫ —Å {current_page} —Å—Ç—Ä–∞–Ω–∏—Ü!")
            
            return business_urls
            
        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ –≤ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {e}")
            return []

    async def collect_links_from_current_page(self):
        """–°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        links = set()
        
        # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        await self.random_delay(2, 4)
        
        link_selectors = [
            'a[href*="/firm/"]',
            'a[href*="/organization/"]',
            'a[href*="/branch/"]',
            'a[href*="astana/firm"]',
            'a[href*="astana/organization"]',
            'a[href*="astana/branch"]'
        ]
        
        for selector in link_selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                for element in elements:
                    href = await element.get_attribute('href')
                    if href:
                        if any(word in href for word in ['firm', 'organization', 'branch']):
                            if href.startswith('/'):
                                full_url = f"https://2gis.kz{href}"
                            elif href.startswith('http'):
                                full_url = href
                            else:
                                continue
                            links.add(full_url)
            except:
                continue
        
        return links

    async def go_to_next_page_fixed(self, page_number):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            logger.info(f"üîç –ò—â–µ–º –∫–Ω–æ–ø–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}...")
            
            # –°–Ω–∞—á–∞–ª–∞ —Å–∫—Ä–æ–ª–ª–∏–º –≤–Ω–∏–∑ —á—Ç–æ–±—ã –ø–∞–≥–∏–Ω–∞—Ü–∏—è –±—ã–ª–∞ –≤–∏–¥–Ω–∞
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await self.random_delay(1, 2)
            
            # –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            pagination_selectors = [
                f'a:has-text("{page_number}")',
                f'button:has-text("{page_number}")',
                f'[data-page="{page_number}"]',
            ]
            
            pagination_element = None
            
            # –ò—â–µ–º —Ç–æ—á–Ω—É—é –∫–Ω–æ–ø–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            for selector in pagination_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    for element in elements:
                        text = await element.text_content()
                        if text and text.strip() == str(page_number):
                            is_visible = await element.is_visible()
                            if is_visible:
                                pagination_element = element
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {selector}")
                                break
                    if pagination_element:
                        break
                except:
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω—É—é –∫–Ω–æ–ø–∫—É, –∏—â–µ–º –∫–Ω–æ–ø–∫—É "–°–ª–µ–¥—É—é—â–∞—è" –∏–ª–∏ ">"
            if not pagination_element:
                logger.info(f"üîç –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞ {page_number}, –∏—â–µ–º '–°–ª–µ–¥—É—é—â–∞—è'...")
                
                next_selectors = [
                    'a:has-text(">")',
                    'button:has-text(">")', 
                    'a:has-text("–°–ª–µ–¥—É—é—â–∞—è")',
                    'button:has-text("–°–ª–µ–¥—É—é—â–∞—è")',
                    '[class*="next"]',
                    '[aria-label*="Next"]',
                    '[aria-label*="–°–ª–µ–¥—É—é—â–∞—è"]'
                ]
                
                for selector in next_selectors:
                    try:
                        element = await self.page.query_selector(selector)
                        if element:
                            is_visible = await element.is_visible()
                            is_enabled = await element.is_enabled()
                            if is_visible and is_enabled:
                                pagination_element = element
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞ '–°–ª–µ–¥—É—é—â–∞—è': {selector}")
                                break
                    except:
                        continue
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —ç–ª–µ–º–µ–Ω—Ç - –∫–ª–∏–∫–∞–µ–º
            if pagination_element:
                try:
                    # –°–∫—Ä–æ–ª–ª–∏–º –∫ —ç–ª–µ–º–µ–Ω—Ç—É
                    await pagination_element.scroll_into_view_if_needed()
                    await self.random_delay(1, 2)
                    
                    # –ö–ª–∏–∫–∞–µ–º
                    await pagination_element.click()
                    logger.info(f"üéØ –ö–ª–∏–∫–Ω—É–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}")
                    
                    # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
                    await self.random_delay(4, 6)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å
                    await self.page.wait_for_load_state('networkidle', timeout=10000)
                    
                    return True
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∏–∫–µ: {e}")
                    return False
            else:
                # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—â–µ–º –í–°–ï —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                logger.info("üîç –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏...")
                
                try:
                    # –ò—â–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                    all_pagination = await self.page.query_selector_all('a, button')
                    
                    for element in all_pagination:
                        try:
                            text = await element.text_content()
                            if text and text.strip() == str(page_number):
                                is_visible = await element.is_visible()
                                is_enabled = await element.is_enabled()
                                
                                if is_visible and is_enabled:
                                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –≤ –æ–±—â–µ–º –ø–æ–∏—Å–∫–µ!")
                                    await element.scroll_into_view_if_needed()
                                    await self.random_delay(1, 2)
                                    await element.click()
                                    await self.random_delay(4, 6)
                                    await self.page.wait_for_load_state('networkidle', timeout=10000)
                                    return True
                        except:
                            continue
                            
                except:
                    pass
                
                logger.info(f"‚ùå –ö–Ω–æ–ø–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
                
        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {e}")
            return False
            
    async def extract_business_info(self, url: str, category: str) -> Optional[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        try:
            logger.info(f"–ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            await self.page.goto(url, wait_until="domcontentloaded", timeout=20000)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            await self.wait_for_dynamic_content()
            
            # –°–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            name = None
            try:
                name = await self.extract_text_by_selectors([
                    'h1', 'h2', '[class*="title"]', '[class*="name"]', '[class*="header"]'
                ])
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏—è: {e}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏ –ª–∏ –º—ã —É–∂–µ —ç—Ç—É –∫–æ–º–ø–∞–Ω–∏—é
            if name and self.is_company_already_processed(name):
                logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç: {name}")
                return None
            
            # –ï—Å–ª–∏ –∫–æ–º–ø–∞–Ω–∏—è –Ω–æ–≤–∞—è, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            address = None
            phone = None
            website = None
            whatsapp = None
            instagram = None
            
            try:
                address = await self.extract_address()
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞–¥—Ä–µ—Å–∞: {e}")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∞–¥—Ä–µ—Å—É (–µ—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –æ–±—â–µ–µ)
            if name and address and self.is_company_already_processed(name, address):
                logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ –∞–¥—Ä–µ—Å—É: {name} - {address}")
                return None
            
            try:
                phone = await self.extract_phone()
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–∞: {e}")
            
            try:
                website = await self.extract_website()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å–∞–π—Ç–∞: {e}")
            
            try:
                whatsapp = await self.extract_whatsapp()
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ WhatsApp: {e}")
            
            try:
                instagram = await self.extract_instagram()
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ Instagram: {e}")
            
            result = {
                '–ù–∞–∑–≤–∞–Ω–∏–µ': name or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
                '–ê–¥—Ä–µ—Å': address or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
                '–¢–µ–ª–µ—Ñ–æ–Ω': phone or '–ù–µ —É–∫–∞–∑–∞–Ω–æ', 
                '–°–∞–π—Ç': website or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
                'WhatsApp': whatsapp or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
                'Instagram': instagram or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
                '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': category,
                '–ï—Å—Ç—å —Å–∞–π—Ç': '–î–∞' if website and website != '–ù–µ —É–∫–∞–∑–∞–Ω–æ' else '–ù–µ—Ç'
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–ø–∞–Ω–∏—é –≤ —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö
            if name:
                self.add_company_to_processed(name, result)
            
            logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {result['–ù–∞–∑–≤–∞–Ω–∏–µ']}, {result['–ê–¥—Ä–µ—Å']}, {result['–¢–µ–ª–µ—Ñ–æ–Ω']}")
            if website and website != '–ù–µ —É–∫–∞–∑–∞–Ω–æ':
                logger.info(f"üåê –°–∞–π—Ç: {result['–°–∞–π—Ç']}")
            if whatsapp and whatsapp != '–ù–µ —É–∫–∞–∑–∞–Ω–æ':
                logger.info(f"üì± WhatsApp: {result['WhatsApp']}")
            if instagram and instagram != '–ù–µ —É–∫–∞–∑–∞–Ω–æ':
                logger.info(f"üì∏ Instagram: {result['Instagram']}")
            
            return result
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å {url}: {e}")
            return None
            
    async def extract_text_by_selectors(self, selectors: List[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å–ø–∏—Å–∫—É —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤"""
        for selector in selectors:
            try:
                element = await self.page.query_selector(selector)
                if element:
                    text = await element.text_content()
                    if text and text.strip():
                        return text.strip()
            except:
                continue
        return None
        
    async def extract_address(self) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        address_selectors = [
            '[class*="address"]',
            '[class*="location"]', 
            '.address',
            '.location'
        ]
        
        address = await self.extract_text_by_selectors(address_selectors)
        if address:
            return address
            
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        try:
            page_text = await self.page.text_content('body')
            if page_text:
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–¥—Ä–µ—Å–∞
                address_patterns = [
                    r'–ñ–ö\s+[–ê-–Ø–∞-—è\s]+,\s*—É–ª–∏—Ü–∞\s+[–ê-–Ø–∞-—è\s]+,\s*\d+',
                    r'—É–ª–∏—Ü–∞\s+[–ê-–Ø–∞-—è\s]+,\s*\d+[–ê-–Ø–∞-—è\s]*',
                    r'–ø—Ä–æ—Å–ø–µ–∫—Ç\s+[–ê-–Ø–∞-—è\s]+,\s*\d+[–ê-–Ø–∞-—è\s]*',
                    r'–±—É–ª—å–≤–∞—Ä\s+[–ê-–Ø–∞-—è\s]+,\s*\d+[–ê-–Ø–∞-—è\s]*',
                    r'[–ê-–Ø–∞-—è\s]+(—Ä–∞–π–æ–Ω|–º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω)[–ê-–Ø–∞-—è\s\d,]*',
                    r'–ê—Å—Ç–∞–Ω–∞[,\s]+[–ê-–Ø–∞-—è\s\d,]+'
                ]
                
                for pattern in address_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        return match.group().strip()
        except:
            pass
            
        return None
        
    async def extract_phone(self) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        # –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ –∏ —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏
        phone_selectors = [
            'a[href^="tel:"]',
            '[class*="phone"]',
            'button[class*="phone"]'
        ]
        
        for selector in phone_selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                for element in elements:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º href
                    href = await element.get_attribute('href')
                    if href and href.startswith('tel:'):
                        return href.replace('tel:', '').strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç
                    text = await element.text_content()
                    if text and re.search(r'[\d\-\+\(\)\s]{7,}', text):
                        return text.strip()
            except:
                continue
                
        # –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        try:
            page_text = await self.page.text_content('body')
            if page_text:
                phone_patterns = [
                    r'\+7\s*[-\(\s]*\d{3}\s*[-\)\s]*\d{3}[-\s]*\d{2}[-\s]*\d{2}',
                    r'8\s*[-\(\s]*\d{3}\s*[-\)\s]*\d{3}[-\s]*\d{2}[-\s]*\d{2}',
                    r'\+7\d{10}',
                    r'8\d{10}'
                ]
                
                for pattern in phone_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        return match.group().strip()
        except:
            pass
            
        return None

    async def wait_for_dynamic_content(self):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            logger.debug("–ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å–µ—Ç–∏
            await self.page.wait_for_load_state('networkidle', timeout=15000)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è JavaScript
            await asyncio.sleep(5)
            
            # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            key_selectors = [
                'h1, h2',  # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                '[class*="contact"]',  # –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                '[class*="phone"]',   # –¢–µ–ª–µ—Ñ–æ–Ω
                'button, a'  # –ö–Ω–æ–ø–∫–∏ –∏ —Å—Å—ã–ª–∫–∏
            ]
            
            for selector in key_selectors:
                try:
                    await self.page.wait_for_selector(selector, timeout=3000)
                    logger.debug(f"–ù–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç: {selector}")
                    break
                except:
                    continue
            
            # –ï—â–µ –Ω–µ–º–Ω–æ–≥–æ –ø–æ–¥–æ–∂–¥–µ–º –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            await asyncio.sleep(2)
            
            logger.debug("–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")
            
        except Exception as e:
            logger.debug(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–∂–∏–¥–∞–Ω–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")

    async def decode_2gis_website_link(self, link: str) -> Optional[str]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ 2–ì–ò–° —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∞–π—Ç–æ–≤"""
        try:
            import base64
            import urllib.parse
            
            if 'link.2gis.com' not in link:
                return None
                
            logger.debug(f"–î–µ–∫–æ–¥–∏—Ä—É–µ–º 2–ì–ò–° —Å—Å—ã–ª–∫—É: {link}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
            parts = link.split('/')
            if len(parts) < 2:
                return None
                
            encoded_part = parts[-1]
            
            # –£–±–∏—Ä–∞–µ–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if '?' in encoded_part:
                encoded_part = encoded_part.split('?')[0]
            if '#' in encoded_part:
                encoded_part = encoded_part.split('#')[0]
                
            try:
                # –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ padding
                for padding in ['', '=', '==', '===']:
                    try:
                        padded_data = encoded_part + padding
                        decoded_bytes = base64.b64decode(padded_data)
                        decoded_string = decoded_bytes.decode('utf-8')
                        
                        logger.debug(f"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞: {decoded_string[:200]}...")
                        
                        # –ò—â–µ–º URL –≤ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
                        url_patterns = [
                            r'https?://([a-zA-Z0-9][-a-zA-Z0-9]*\.(?:kz|com|ru|org|net|biz|cafe|coffee)(?:/[^\s]*)?)',
                            r'http://([a-zA-Z0-9][-a-zA-Z0-9]*\.(?:kz|com|ru|org|net|biz|cafe|coffee)(?:/[^\s]*)?)',
                            r'([a-zA-Z0-9][-a-zA-Z0-9]*\.(?:kz|com|ru|org|net|biz|cafe|coffee)(?:/[^\s]*)?)'
                        ]
                        
                        for pattern in url_patterns:
                            matches = re.findall(pattern, decoded_string, re.IGNORECASE)
                            for match in matches:
                                domain = match if isinstance(match, str) else match[0]
                                
                                # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –¥–æ–º–µ–Ω—ã
                                if not any(bad in domain.lower() for bad in ['2gis', 'sberbank', 'yandex', 'google']):
                                    if len(domain) > 6:
                                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –ø—Ä–æ—Ç–æ–∫–æ–ª
                                        if domain.startswith('http'):
                                            result = domain
                                        else:
                                            result = f"https://{domain}"
                                        logger.info(f"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω —Å–∞–π—Ç: {result}")
                                        return result
                        
                        break  # –ï—Å–ª–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ, –≤—ã—Ö–æ–¥–∏–º
                    except:
                        continue
                        
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                
            return None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Å—ã–ª–∫–∏: {e}")
            return None

    async def extract_website(self) -> Optional[str]:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∞–π—Ç–∞ –ø–æ SVG –∏–∫–æ–Ω–∫–µ –≥–ª–æ–±—É—Å–∞ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º –∫–ª–∞—Å—Å–∞–º"""
        try:
            logger.info("–ò—â–µ–º —Å–∞–π—Ç –ø–æ SVG –∏–∫–æ–Ω–∫–µ –≥–ª–æ–±—É—Å–∞...")

            # –ò—â–µ–º SVG —Å –∏–∫–æ–Ω–∫–æ–π –≥–ª–æ–±—É—Å–∞ (–∑–µ–º–ª–∏)
            svg_selectors = [
                'svg[fill="#028eff"]',  # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ü–≤–µ—Ç –∏–∑ –ø—Ä–∏–º–µ—Ä–∞
                'svg',  # –í—Å–µ SVG —ç–ª–µ–º–µ–Ω—Ç—ã
                'div._1iftozu svg'  # SVG –≤–Ω—É—Ç—Ä–∏ div —Å –∫–ª–∞—Å—Å–æ–º _1iftozu
            ]
            
            for svg_selector in svg_selectors:
                try:
                    svg_elements = await self.page.query_selector_all(svg_selector)
                    
                    for svg in svg_elements:
                        try:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∏–∫–æ–Ω–∫–∞ –≥–ª–æ–±—É—Å–∞ –ø–æ path
                            path_element = await svg.query_selector('path')
                            if path_element:
                                path_d = await path_element.get_attribute('d')
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —á–∞—Å—Ç–∏ path –¥–ª—è –∏–∫–æ–Ω–∫–∏ –≥–ª–æ–±—É—Å–∞
                                if path_d and any(pattern in path_d for pattern in ['M12 4a8 8', 'a8 8 0', 'A6 6 0']):
                                    logger.debug("–ù–∞–π–¥–µ–Ω–∞ SVG –∏–∫–æ–Ω–∫–∞ –≥–ª–æ–±—É—Å–∞")
                                    
                                    # –ò—â–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Å–∞–π—Ç–æ–º
                                    current_element = svg
                                    for level in range(5):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 5 —É—Ä–æ–≤–Ω–µ–π –≤–≤–µ—Ä—Ö
                                        try:
                                            parent = await current_element.query_selector('xpath=..')
                                            if not parent:
                                                break
                                                
                                            # –ò—â–µ–º div —Å –∫–ª–∞—Å—Å–æ–º _49kxlr —Ä—è–¥–æ–º –∏–ª–∏ –≤–Ω—É—Ç—Ä–∏
                                            website_containers = await parent.query_selector_all('div._49kxlr, ._49kxlr')
                                            
                                            for container in website_containers:
                                                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                                                links = await container.query_selector_all('a')
                                                for link in links:
                                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º href
                                                    href = await link.get_attribute('href')
                                                    if href and 'link.2gis.com' in href:
                                                        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º 2–ì–ò–° —Å—Å—ã–ª–∫—É
                                                        decoded_site = await self.decode_2gis_website_link(href)
                                                        if decoded_site:
                                                            logger.info(f"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω —Å–∞–π—Ç: {decoded_site}")
                                                            return decoded_site
                                                    
                                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ–º–µ–Ω–æ–º)
                                                    link_text = await link.text_content()
                                                    if link_text:
                                                        link_text = link_text.strip()
                                                        logger.debug(f"–ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏: {link_text}")
                                                        
                                                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–æ–º–µ–Ω (–≤–∫–ª—é—á–∞—è –ø–æ–¥–¥–æ–º–µ–Ω—ã)
                                                        if self.is_valid_domain(link_text):
                                                            # –ù–ï —É–±–∏—Ä–∞–µ–º –ø–æ–¥–¥–æ–º–µ–Ω—ã - –±–µ—Ä–µ–º –∫–∞–∫ –µ—Å—Ç—å
                                                            if not link_text.startswith('http'):
                                                                result = f"https://{link_text}"
                                                            else:
                                                                result = link_text
                                                            logger.info(f"–ù–∞–π–¥–µ–Ω —Å–∞–π—Ç: {result}")
                                                            return result
                                            
                                            current_element = parent
                                        except:
                                            break
                        except:
                            continue
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ SVG {svg_selector}: {e}")
                    continue

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ SVG, –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
            logger.info("–ü–æ–∏—Å–∫ –ø–æ SVG –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—â–µ–º –Ω–∞–ø—Ä—è–º—É—é –ø–æ –∫–ª–∞—Å—Å–∞–º...")
            
            try:
                # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–æ–º _49kxlr
                website_elements = await self.page.query_selector_all('._49kxlr, div._49kxlr')
                logger.debug(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –∫–ª–∞—Å—Å–æ–º _49kxlr: {len(website_elements)}")
                
                for element in website_elements:
                    try:
                        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏
                        links = await element.query_selector_all('a')
                        for link in links:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º href –Ω–∞ 2gis —Å—Å—ã–ª–∫—É
                            href = await link.get_attribute('href')
                            if href and 'link.2gis.com' in href:
                                decoded_site = await self.decode_2gis_website_link(href)
                                if decoded_site:
                                    logger.info(f"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω —Å–∞–π—Ç –∏–∑ _49kxlr: {decoded_site}")
                                    return decoded_site
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏
                            link_text = await link.text_content()
                            if link_text:
                                link_text = link_text.strip()
                                
                                if self.is_valid_domain(link_text):
                                    if not link_text.startswith('http'):
                                        result = f"https://{link_text}"
                                    else:
                                        result = link_text
                                    logger.info(f"–ù–∞–π–¥–µ–Ω —Å–∞–π—Ç –≤ _49kxlr: {result}")
                                    return result
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–ª–µ–º–µ–Ω—Ç–∞ _49kxlr: {e}")
                        continue
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –∫–ª–∞—Å—Å—É _49kxlr: {e}")

            logger.info("–°–∞–π—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
                    
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–∞–π—Ç–∞: {e}")
            return None

    def is_valid_domain(self, domain: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —Å—Ç—Ä–æ–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–ª–∏–¥–Ω—ã–º –¥–æ–º–µ–Ω–æ–º (–≤–∫–ª—é—á–∞—è –ø–æ–¥–¥–æ–º–µ–Ω—ã)"""
        try:
            if not domain:
                return False
                
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª –µ—Å–ª–∏ –µ—Å—Ç—å
            domain = domain.replace('https://', '').replace('http://', '')
            
            # –£–±–∏—Ä–∞–µ–º –ø—É—Ç—å –µ—Å–ª–∏ –µ—Å—Ç—å
            domain = domain.split('/')[0]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–æ–º–µ–Ω–∞ (–≤–∫–ª—é—á–∞—è –ø–æ–¥–¥–æ–º–µ–Ω—ã)
            domain_pattern = r'^([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$'
            
            if re.match(domain_pattern, domain):
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
                parts = domain.split('.')
                
                # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–∏–Ω–∏–º—É–º 2 —á–∞—Å—Ç–∏ (–¥–æ–º–µ–Ω.–∑–æ–Ω–∞)
                if len(parts) < 2:
                    return False
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–æ–Ω—É (–ø–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å)
                tld = parts[-1].lower()
                valid_tlds = ['com', 'ru', 'kz', 'org', 'net', 'biz', 'info', 'cafe', 'coffee', 'shop', 'store']
                
                if tld in valid_tlds:
                    # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –¥–æ–º–µ–Ω—ã
                    excluded_domains = ['2gis', 'google', 'yandex', 'facebook', 'vk']
                    if not any(excluded in domain.lower() for excluded in excluded_domains):
                        return True
            
            return False
            
        except:
            return False
        
    async def decode_2gis_link(self, link: str) -> Optional[str]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Å—ã–ª–æ–∫ 2–ì–ò–° –¥–ª—è WhatsApp"""
        try:
            import base64
            import urllib.parse
            
            if 'link.2gis.com' not in link:
                return None
                
            logger.debug(f"–ü—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å: {link}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º base64 —á–∞—Å—Ç—å –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ /
            parts = link.split('/')
            if len(parts) < 2:
                return None
                
            encoded_part = parts[-1]
            
            # –£–¥–∞–ª—è–µ–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å
            if '?' in encoded_part:
                encoded_part = encoded_part.split('?')[0]
            if '#' in encoded_part:
                encoded_part = encoded_part.split('#')[0]
                
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
                for padding in ['', '=', '==', '===']:
                    try:
                        padded_data = encoded_part + padding
                        decoded_bytes = base64.b64decode(padded_data)
                        decoded_string = decoded_bytes.decode('utf-8')
                        
                        # –ò—â–µ–º wa.me —Å—Å—ã–ª–∫—É –≤ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
                        wa_patterns = [
                            r'(https://wa\.me/[^\s%"\']+)',
                            r'wa\.me/(\d+)',
                            r'whatsapp://send\?phone=(\d+)'
                        ]
                        
                        for pattern in wa_patterns:
                            matches = re.findall(pattern, decoded_string)
                            for match in matches:
                                if match.startswith('https://'):
                                    wa_url = urllib.parse.unquote(match)
                                    logger.info(f"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp: {wa_url}")
                                    return wa_url
                                elif match.isdigit():
                                    wa_url = f"https://wa.me/{match}"
                                    logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp: {wa_url}")
                                    return wa_url
                        
                        break  # –ï—Å–ª–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ, –≤—ã—Ö–æ–¥–∏–º
                    except:
                        continue
                        
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è base64: {e}")
                
            return None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Å—ã–ª–∫–∏ 2gis: {e}")
            return None

    async def extract_whatsapp(self) -> Optional[str]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ WhatsApp"""
        try:
            # 1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ WhatsApp
            direct_selectors = [
                'a[href*="wa.me"]',
                'a[href*="whatsapp"]',
                'a[href*="link.2gis.com"]'
            ]
            
            for selector in direct_selectors:
                try:
                    links = await self.page.query_selector_all(selector)
                    for link in links:
                        href = await link.get_attribute('href')
                        if href:
                            if 'wa.me' in href or 'whatsapp' in href:
                                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –ø—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ WhatsApp: {href}")
                                return href
                            elif 'link.2gis.com' in href:
                                # –ü—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Å—Å—ã–ª–∫—É 2gis
                                decoded = await self.decode_2gis_link(href)
                                if decoded:
                                    return decoded
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ {selector}: {e}")
                    continue

            # 2. –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ WhatsApp –ë–ï–ó –ö–õ–ò–ö–ê - —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã
            whatsapp_selectors = [
                'button[title*="WhatsApp"], button[title*="whatsapp"]',
                'a[title*="WhatsApp"], a[title*="whatsapp"]',
                'div[title*="WhatsApp"], div[title*="whatsapp"]',
                '[class*="whatsapp"]',
                '[data-social="whatsapp"]',
                'button:has-text("WhatsApp")',
                'a:has-text("WhatsApp")'
            ]
            
            for selector in whatsapp_selectors:
                try:
                    buttons = await self.page.query_selector_all(selector)
                    for button in buttons:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –ë–ï–ó –ö–õ–ò–ö–ê
                        attributes_to_check = [
                            'href', 'data-url', 'data-link', 'data-phone', 
                            'data-action', 'data-contact', 'onclick',
                            'data-whatsapp', 'data-phone-number'
                        ]
                        
                        for attr in attributes_to_check:
                            try:
                                attr_value = await button.get_attribute(attr)
                                if not attr_value:
                                    continue
                                    
                                # –ò—â–µ–º –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –≤ –∞—Ç—Ä–∏–±—É—Ç–µ
                                phone_patterns = [
                                    r'(\+?7\d{10})',
                                    r'wa\.me/(\d+)',
                                    r'whatsapp://send\?phone=(\d+)'
                                ]
                                
                                for pattern in phone_patterns:
                                    match = re.search(pattern, attr_value)
                                    if match:
                                        phone = match.group(1).replace('+', '')
                                        if not phone.startswith('7') and len(phone) == 10:
                                            phone = '7' + phone
                                        elif not phone.startswith('7') and len(phone) == 11:
                                            phone = phone
                                        result = f"https://wa.me/{phone}"
                                        logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp –∏–∑ {attr}: {result}")
                                        return result
                                
                                # –ò—â–µ–º –≥–æ—Ç–æ–≤—É—é —Å—Å—ã–ª–∫—É WhatsApp
                                if 'wa.me' in attr_value or 'whatsapp' in attr_value:
                                    wa_match = re.search(r'(https://wa\.me/[^\s\'"]+)', attr_value)
                                    if wa_match:
                                        logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp –≤ {attr}: {wa_match.group(1)}")
                                        return wa_match.group(1)
                            except Exception as e:
                                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∞—Ç—Ä–∏–±—É—Ç–∞ {attr}: {e}")
                                continue
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ {selector}: {e}")
                    continue

            # 3. –ò—â–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            try:
                page_content = await self.page.content()
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ 2gis —Å –≤–æ–∑–º–æ–∂–Ω—ã–º WhatsApp
                gis_patterns = [
                    r'href=["\']([^"\']*link\.2gis\.com[^"\']*)["\']',
                    r'(https://link\.2gis\.com/[^\s\'"]+)'
                ]
                
                for pattern in gis_patterns:
                    matches = re.findall(pattern, page_content)
                    for match in matches:
                        decoded = await self.decode_2gis_link(match)
                        if decoded:
                            return decoded
                
                # –ò—â–µ–º –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ WhatsApp
                whatsapp_patterns = [
                    r'whatsapp[^0-9]*(\+?7\d{10})',
                    r'wa\.me/(\d+)',
                    r'data-phone["\']:\s*["\'](\+?7\d{10})["\']',
                    r'phone["\']:\s*["\'](\+?7\d{10})["\']',
                    r'whatsapp["\']?\s*:\s*["\']([^"\']+)["\']'
                ]
                
                for pattern in whatsapp_patterns:
                    matches = re.findall(pattern, page_content, re.IGNORECASE)
                    for match in matches:
                        if 'wa.me' in match or 'whatsapp' in match:
                            logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp –≤ –∫–æ–¥–µ: {match}")
                            return match
                        elif re.match(r'\+?7?\d{10,11}', match):
                            phone = match.replace('+', '').replace(' ', '').replace('-', '')
                            if len(phone) == 11 and phone.startswith('7'):
                                result = f"https://wa.me/{phone}"
                            elif len(phone) == 10:
                                result = f"https://wa.me/7{phone}"
                            else:
                                continue
                            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞: {result}")
                            return result
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ: {e}")

            # 4. –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—â–µ–º –ª—é–±—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –Ω–æ–º–µ—Ä–æ–≤ —Ä—è–¥–æ–º —Å WhatsApp –≤ —Ç–µ–∫—Å—Ç–µ
            try:
                page_text = await self.page.text_content('body')
                if page_text:
                    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º WhatsApp
                    text_lower = page_text.lower()
                    whatsapp_pos = text_lower.find('whatsapp')
                    
                    if whatsapp_pos != -1:
                        # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –≤ —Ä–∞–¥–∏—É—Å–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç —Å–ª–æ–≤–∞ WhatsApp
                        start = max(0, whatsapp_pos - 100)
                        end = min(len(page_text), whatsapp_pos + 100)
                        context = page_text[start:end]
                        
                        # –ò—â–µ–º –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –≤ —ç—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
                        phone_match = re.search(r'(\+?7\d{10})', context)
                        if phone_match:
                            phone = phone_match.group(1).replace('+', '')
                            if not phone.startswith('7'):
                                phone = '7' + phone
                            result = f"https://wa.me/{phone}"
                            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {result}")
                            return result
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ —Ç–µ–∫—Å—Ç–µ: {e}")
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ WhatsApp: {e}")
            
        return None
        
    async def extract_instagram(self) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Instagram - —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ Instagram
            instagram_links = await self.page.query_selector_all('a[href*="instagram"]')
            for link in instagram_links:
                href = await link.get_attribute('href')
                if href and 'instagram' in href:
                    logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ Instagram: {href}")
                    return href
                    
            # –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ Instagram –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–±—ã—Ç–∏—è
            instagram_buttons = await self.page.query_selector_all('button, div, span, a')
            for button in instagram_buttons:
                try:
                    text = await button.text_content()
                    if text and 'instagram' in text.lower():
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º onclick
                        onclick = await button.get_attribute('onclick')
                        if onclick:
                            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤ onclick
                            instagram_match = re.search(r'(https://[^\'"\s]*instagram\.com[^\'"\s]*)', onclick)
                            if instagram_match:
                                link = instagram_match.group(1)
                                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ Instagram –≤ onclick: {link}")
                                return link
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º data-–∞—Ç—Ä–∏–±—É—Ç—ã
                        for attr in ['data-url', 'data-link', 'data-href', 'data-instagram', 'data-action']:
                            attr_value = await button.get_attribute(attr)
                            if attr_value and 'instagram' in attr_value:
                                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ Instagram –≤ {attr}: {attr_value}")
                                return attr_value
                        
                        # –ò—â–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Å—Å—ã–ª–∫–∞–º–∏
                        parent = button
                        for _ in range(3):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 3 —É—Ä–æ–≤–Ω–µ–π –≤–≤–µ—Ä—Ö
                            try:
                                parent = await parent.query_selector('xpath=..')
                                if parent:
                                    parent_href = await parent.get_attribute('href')
                                    if parent_href and 'instagram' in parent_href:
                                        logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ Instagram –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ: {parent_href}")
                                        return parent_href
                            except:
                                break
                except:
                    continue
                    
            # –ò—â–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            try:
                page_content = await self.page.content()
                # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å Instagram —Å—Å—ã–ª–∫–∞–º–∏
                js_patterns = [
                    r'instagram["\']?\s*:\s*["\']([^"\']+)["\']',
                    r'instagram\.com/([^"\')\s/]+)',
                    r'https://(?:www\.)?instagram\.com/([^"\')\s/]+)',
                    r'window\.open\(["\']([^"\']*instagram[^"\']*)["\']',
                    r'href\s*=\s*["\']([^"\']*instagram[^"\']*)["\']'
                ]
                
                for pattern in js_patterns:
                    matches = re.findall(pattern, page_content, re.IGNORECASE)
                    for match in matches:
                        if match and 'instagram' not in match:
                            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω —Ç–æ–ª—å–∫–æ username, —Å–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É
                            match = f"https://instagram.com/{match}"
                        elif match and 'instagram' in match and not match.startswith('http'):
                            match = f"https://{match}"
                        
                        if match and 'instagram.com' in match:
                            logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ Instagram –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ: {match}")
                            return match
            except:
                pass
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ Instagram: {e}")
            
        return None
        
    async def save_to_excel(self, filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Excel —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            if not self.results:
                logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                return
                
            df = pd.DataFrame(self.results)
            
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                df.to_excel(writer, sheet_name='–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏', index=False)
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                stats_data = {
                    '–ú–µ—Ç—Ä–∏–∫–∞': [
                        '–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π',
                        '–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ',
                        '–ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤',
                        '–ö–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ',
                        '–ö–æ–º–ø–∞–Ω–∏–π —Å —Å–∞–π—Ç–∞–º–∏',
                        '–ö–æ–º–ø–∞–Ω–∏–π —Å WhatsApp',
                        '–ö–æ–º–ø–∞–Ω–∏–π —Å Instagram'
                    ],
                    '–ó–Ω–∞—á–µ–Ω–∏–µ': [
                        len(self.processed_companies),
                        len(self.results),
                        max(0, len(self.processed_companies) - len(self.results)),
                        len(set(result['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] for result in self.results)),
                        len([r for r in self.results if r['–ï—Å—Ç—å —Å–∞–π—Ç'] == '–î–∞']),
                        len([r for r in self.results if r['WhatsApp'] != '–ù–µ —É–∫–∞–∑–∞–Ω–æ']),
                        len([r for r in self.results if r['Instagram'] != '–ù–µ —É–∫–∞–∑–∞–Ω–æ'])
                    ]
                }
                stats_df = pd.DataFrame(stats_data)
                stats_df.to_excel(writer, sheet_name='–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', index=False)
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                worksheet = writer.sheets['–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏']
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
                    
            logger.info(f"üìä –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filename}")
            logger.info(f"üìà –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(self.results)}")
            logger.info(f"üîÑ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π: {len(self.processed_companies)}")
            logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {max(0, len(self.processed_companies) - len(self.results))}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Excel: {e}")

    def get_deduplication_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        return {
            'unique_companies': len(self.processed_companies),
            'total_records': len(self.results),
            'duplicates_skipped': max(0, len(self.processed_companies) - len(self.results)),
            'categories_processed': len(set(result['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] for result in self.results)),
            'companies_with_websites': len([r for r in self.results if r['–ï—Å—Ç—å —Å–∞–π—Ç'] == '–î–∞']),
            'companies_with_whatsapp': len([r for r in self.results if r['WhatsApp'] != '–ù–µ —É–∫–∞–∑–∞–Ω–æ']),
            'companies_with_instagram': len([r for r in self.results if r['Instagram'] != '–ù–µ —É–∫–∞–∑–∞–Ω–æ'])
        }
            
    async def parse_category(self, category: str):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π"""
        try:
            logger.info(f"üéØ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            if not await self.open_2gis_and_search(category):
                return
                
            # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤—Å–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
            business_urls = await self.get_business_links_pagination_fixed()
            
            if not business_urls:
                logger.warning(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}'")
                return
                
            logger.info(f"üîç –ë—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å {len(business_urls)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
            processed = 0
            skipped = 0
            
            for i, url in enumerate(business_urls, 1):
                try:
                    logger.info(f"üìã –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é {i}/{len(business_urls)}")
                    
                    business_info = await self.extract_business_info(url, category)
                    if business_info:
                        self.results.append(business_info)
                        processed += 1
                        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ: {business_info['–ù–∞–∑–≤–∞–Ω–∏–µ']}")
                    else:
                        skipped += 1
                        logger.info(f"‚è≠Ô∏è –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ (–¥—É–±–ª–∏–∫–∞—Ç –∏–ª–∏ –æ—à–∏–±–∫–∞)")
                    
                    await self.random_delay(2, 4)
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ {i}: {e}")
                    skipped += 1
                    continue
                    
            logger.info(f"üéâ –ó–∞–≤–µ—Ä—à–µ–Ω –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}'")
            logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}, –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}")
            
        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}': {e}")
            
    async def run(self, categories: List[str]):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞ —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π"""
        try:
            logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è –≥–æ—Ä–æ–¥–∞: {self.city}")
            logger.info(f"üìù –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(categories)}")
            logger.info(f"üéØ –ú–∞–∫—Å–∏–º—É–º –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {self.max_items_per_category}")
            
            await self.setup_browser()
            
            for i, category in enumerate(categories, 1):
                logger.info(f"\n{'='*50}")
                logger.info(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è {i}/{len(categories)}: {category}")
                logger.info(f"{'='*50}")
                
                await self.parse_category(category)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                stats = self.get_deduplication_stats()
                logger.info(f"üìä –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
                logger.info(f"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π: {stats['unique_companies']}")
                logger.info(f"   ‚Ä¢ –ó–∞–ø–∏—Å–µ–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ: {stats['total_records']}")
                logger.info(f"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['duplicates_skipped']}")
                
                if i < len(categories):  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
                    logger.info(f"‚è≥ –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π...")
                    await self.random_delay(5, 8)
                
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"2gis_results_{self.city}_{timestamp}.xlsx"
            await self.save_to_excel(filename)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            final_stats = self.get_deduplication_stats()
            logger.info(f"\nüéØ –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            logger.info(f"{'='*40}")
            logger.info(f"üìà –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π: {final_stats['unique_companies']}")
            logger.info(f"üìã –ó–∞–ø–∏—Å–µ–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ: {final_stats['total_records']}")
            logger.info(f"üîÑ –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {final_stats['duplicates_skipped']}")
            logger.info(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {final_stats['categories_processed']}")
            logger.info(f"üåê –ö–æ–º–ø–∞–Ω–∏–π —Å —Å–∞–π—Ç–∞–º–∏: {final_stats['companies_with_websites']}")
            logger.info(f"üì± –ö–æ–º–ø–∞–Ω–∏–π —Å WhatsApp: {final_stats['companies_with_whatsapp']}")
            logger.info(f"üì∏ –ö–æ–º–ø–∞–Ω–∏–π —Å Instagram: {final_stats['companies_with_instagram']}")
            logger.info(f"üíæ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
            logger.info(f"{'='*40}")
            
        except Exception as e:
            logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            raise
            
        finally:
            if self.browser:
                try:
                    await self.browser.close()
                    logger.info("üîí –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")
                except:
                    pass
            if hasattr(self, 'playwright'):
                try:
                    await self.playwright.stop()
                except:
                    pass

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤"""
    parser = argparse.ArgumentParser(
        description='–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä 2–ì–ò–° —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å–∞–π—Ç–æ–≤',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python improved_2gis_parser.py --city "–ê—Å—Ç–∞–Ω–∞" --categories "–∫–æ—Ñ–µ–π–Ω–∏" "—Å–∞–ª–æ–Ω—ã –∫—Ä–∞—Å–æ—Ç—ã" --max-items 50
  python improved_2gis_parser.py --config config.json
  python improved_2gis_parser.py --categories "—Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏" "—Ñ–∏—Ç–Ω–µ—Å-—Ü–µ–Ω—Ç—Ä—ã" "—Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã"
        """
    )
    
    parser.add_argument('--city', '-c', default='–ê—Å—Ç–∞–Ω–∞', 
                       help='–ì–æ—Ä–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –ê—Å—Ç–∞–Ω–∞)')
    parser.add_argument('--categories', '-cat', nargs='+', 
                       default=['–∫–æ—Ñ–µ–π–Ω–∏'],
                       help='–°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –∫–æ—Ñ–µ–π–Ω–∏)')
    parser.add_argument('--max-items', '-m', type=int, default=100,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 100)')
    parser.add_argument('--config', '-cfg', 
                       help='–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ (DEBUG —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)')
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Ä–æ–≤–Ω—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info("üîç –í–∫–ª—é—á–µ–Ω –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ä–µ–∂–∏–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞
    if args.config:
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                config = json.load(f)
                args.city = config.get('city', args.city)
                args.categories = config.get('categories', args.categories)
                args.max_items = config.get('max_items', args.max_items)
                logger.info(f"üìÑ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {args.config}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    if args.max_items <= 0:
        logger.error("‚ùå –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ 0")
        return
        
    if not args.categories:
        logger.error("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –∫–∞—Ç–µ–≥–æ—Ä–∏—é")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞
    logger.info(f"üéØ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞...")
    parser_instance = GISParser(
        city=args.city, 
        max_items_per_category=args.max_items
    )
    
    try:
        logger.info(f"‚ñ∂Ô∏è –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
        asyncio.run(parser_instance.run(args.categories))
        logger.info("üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        
    except Exception as e:
        logger.error(f"üí• –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —Å –æ—à–∏–±–∫–æ–π: {e}")
        return 1
        
    return 0

if __name__ == "__main__":
    exit(main())