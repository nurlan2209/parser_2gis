import asyncio
import logging
import random
import re
from datetime import datetime
from typing import Dict, List, Optional
import pandas as pd
from playwright.async_api import async_playwright, Page, Browser
import argparse
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('2gis_parser.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class GISParser:
    def __init__(self, city: str = "–ê—Å—Ç–∞–Ω–∞", max_items_per_category: int = 100):
        self.city = city
        self.max_items_per_category = max_items_per_category
        self.results = []
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        
    async def setup_browser(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∑–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞"""
        self.playwright = await async_playwright().start()
        
        self.browser = await self.playwright.chromium.launch(
            headless=False,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-features=VizDisplayCompositor',
                '--ignore-certificate-errors',
                '--ignore-ssl-errors',
                '--ignore-certificate-errors-spki-list'
            ]
        )
        
        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 920},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            java_script_enabled=True,
            ignore_https_errors=True
        )
        
        self.page = await context.new_page()
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç—ã
        self.page.set_default_timeout(30000)
        self.page.set_default_navigation_timeout(30000)
        
        # –ë–ª–æ–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç—è–∂–µ–ª—ã–µ —Ä–µ—Å—É—Ä—Å—ã
        await self.page.route("**/*.{png,jpg,jpeg,gif,webp,svg,mp4,avi,mov}", lambda route: route.abort())
        
        logger.info("–ë—Ä–∞—É–∑–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω")
        
    async def random_delay(self, min_sec: float = 1.0, max_sec: float = 3.0):
        """–°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞"""
        delay = random.uniform(min_sec, max_sec)
        logger.debug(f"–ó–∞–¥–µ—Ä–∂–∫–∞ {delay:.1f} —Å–µ–∫—É–Ω–¥")
        await asyncio.sleep(delay)
        
    async def open_2gis_and_search(self, category: str):
        """–û—Ç–∫—Ä—ã—Ç–∏–µ 2–ì–ò–° –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã URL
            urls_to_try = [
                f"https://2gis.kz/{self.city.lower()}",
                f"https://2gis.kz/astana",
                "https://2gis.kz"
            ]
            
            page_loaded = False
            for url in urls_to_try:
                try:
                    logger.info(f"–ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å: {url}")
                    await self.page.goto(url, wait_until="domcontentloaded", timeout=20000)
                    await self.random_delay(2, 4)
                    page_loaded = True
                    logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
                    break
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}: {e}")
                    continue
            
            if not page_loaded:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É 2–ì–ò–°")
                return False
            
            # –ò—â–µ–º –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏
            search_selectors = [
                'input[placeholder*="–ü–æ–∏—Å–∫"]',
                'input[placeholder*="–ø–æ–∏—Å–∫"]',
                'input[type="search"]',
                'input[name="search"]',
                '.search-input input',
                'input'
            ]
            
            search_input = None
            for selector in search_selectors:
                try:
                    search_input = await self.page.wait_for_selector(selector, timeout=5000)
                    if search_input:
                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞: {selector}")
                        break
                except:
                    continue
            
            if not search_input:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞")
                return False
            
            # –í–≤–æ–¥–∏–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            await search_input.fill('')
            await self.random_delay(0.5, 1)
            await search_input.type(category, delay=100)
            await self.random_delay(1, 2)
            
            # –ù–∞–∂–∏–º–∞–µ–º Enter
            await self.page.keyboard.press('Enter')
            await self.random_delay(5, 7)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è
            current_url = self.page.url
            if category.lower() in current_url.lower() or 'search' in current_url.lower():
                logger.info(f"–ü–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ: {category}")
                return True
            else:
                logger.warning(f"–ü–æ–∏—Å–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω. –¢–µ–∫—É—â–∏–π URL: {current_url}")
                # –î–∞–µ–º –µ—â–µ –æ–¥–Ω—É –ø–æ–ø—ã—Ç–∫—É
                await self.random_delay(3, 5)
                return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return False
            
    async def get_business_links_pagination_fixed(self):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ø–∞–≥–∏–Ω–∞—Ü–∏—è - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            await self.random_delay(3, 5)
            
            logger.info("üîç –ó–∞–ø—É—Å–∫ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –ø–∞–≥–∏–Ω–∞—Ü–∏–∏...")
            
            all_unique_links = set()
            current_page = 1
            max_pages = 50  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç
            consecutive_failures = 0
            max_failures = 3  # –ï—Å–ª–∏ 3 —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥ –Ω–µ –º–æ–∂–µ–º –ø–µ—Ä–µ–π—Ç–∏ - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
            
            while current_page <= max_pages and consecutive_failures < max_failures:
                logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {current_page}")
                
                # –°–∫—Ä–æ–ª–ª–∏–º –≤–≤–µ—Ä—Ö –ø–µ—Ä–µ–¥ —Å–±–æ—Ä–æ–º —Å—Å—ã–ª–æ–∫
                await self.page.evaluate("window.scrollTo(0, 0)")
                await self.random_delay(2, 3)
                
                # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                current_links = await self.collect_links_from_current_page()
                old_count = len(all_unique_links)
                all_unique_links.update(current_links)
                new_count = len(all_unique_links)
                new_links = new_count - old_count
                
                logger.info(f"üìä –°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page}: –¥–æ–±–∞–≤–ª–µ–Ω–æ {new_links} –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ (–≤—Å–µ–≥–æ: {new_count})")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
                if new_count >= self.max_items_per_category:
                    logger.info(f"üéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç {self.max_items_per_category} —Å—Å—ã–ª–æ–∫!")
                    break
                
                # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫, –Ω–æ —ç—Ç–æ –Ω–µ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –≤–æ–∑–º–æ–∂–Ω–æ –∫–æ–Ω–µ—Ü
                if new_links == 0 and current_page > 1:
                    logger.info("‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ - –≤–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
                    break
                
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                next_page_found = await self.go_to_next_page_fixed(current_page + 1)
                
                if next_page_found:
                    consecutive_failures = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –Ω–µ—É–¥–∞—á
                    current_page += 1
                else:
                    consecutive_failures += 1
                    logger.info(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {current_page + 1} (–ø–æ–ø—ã—Ç–∫–∞ {consecutive_failures}/{max_failures})")
                    
                    if consecutive_failures >= max_failures:
                        logger.info("üèÅ –ë–æ–ª—å—à–µ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
                        break
            
            business_urls = list(all_unique_links)[:self.max_items_per_category]
            logger.info(f"üéâ –ò–¢–û–ì–û —Å–æ–±—Ä–∞–Ω–æ {len(business_urls)} —Å—Å—ã–ª–æ–∫ —Å {current_page} —Å—Ç—Ä–∞–Ω–∏—Ü!")
            
            return business_urls
            
        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ –≤ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {e}")
            return []

    async def collect_links_from_current_page(self):
        """–°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        links = set()
        
        # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        await self.random_delay(2, 4)
        
        link_selectors = [
            'a[href*="/firm/"]',
            'a[href*="/organization/"]',
            'a[href*="/branch/"]',
            'a[href*="astana/firm"]',
            'a[href*="astana/organization"]',
            'a[href*="astana/branch"]'
        ]
        
        for selector in link_selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                for element in elements:
                    href = await element.get_attribute('href')
                    if href:
                        if any(word in href for word in ['firm', 'organization', 'branch']):
                            if href.startswith('/'):
                                full_url = f"https://2gis.kz{href}"
                            elif href.startswith('http'):
                                full_url = href
                            else:
                                continue
                            links.add(full_url)
            except:
                continue
        
        return links

    async def go_to_next_page_fixed(self, page_number):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            logger.info(f"üîç –ò—â–µ–º –∫–Ω–æ–ø–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}...")
            
            # –°–Ω–∞—á–∞–ª–∞ —Å–∫—Ä–æ–ª–ª–∏–º –≤–Ω–∏–∑ —á—Ç–æ–±—ã –ø–∞–≥–∏–Ω–∞—Ü–∏—è –±—ã–ª–∞ –≤–∏–¥–Ω–∞
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await self.random_delay(1, 2)
            
            # –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            pagination_selectors = [
                f'a:has-text("{page_number}")',
                f'button:has-text("{page_number}")',
                f'[data-page="{page_number}"]',
            ]
            
            pagination_element = None
            
            # –ò—â–µ–º —Ç–æ—á–Ω—É—é –∫–Ω–æ–ø–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            for selector in pagination_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    for element in elements:
                        text = await element.text_content()
                        if text and text.strip() == str(page_number):
                            is_visible = await element.is_visible()
                            if is_visible:
                                pagination_element = element
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {selector}")
                                break
                    if pagination_element:
                        break
                except:
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω—É—é –∫–Ω–æ–ø–∫—É, –∏—â–µ–º –∫–Ω–æ–ø–∫—É "–°–ª–µ–¥—É—é—â–∞—è" –∏–ª–∏ ">"
            if not pagination_element:
                logger.info(f"üîç –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞ {page_number}, –∏—â–µ–º '–°–ª–µ–¥—É—é—â–∞—è'...")
                
                next_selectors = [
                    'a:has-text(">")',
                    'button:has-text(">")', 
                    'a:has-text("–°–ª–µ–¥—É—é—â–∞—è")',
                    'button:has-text("–°–ª–µ–¥—É—é—â–∞—è")',
                    '[class*="next"]',
                    '[aria-label*="Next"]',
                    '[aria-label*="–°–ª–µ–¥—É—é—â–∞—è"]'
                ]
                
                for selector in next_selectors:
                    try:
                        element = await self.page.query_selector(selector)
                        if element:
                            is_visible = await element.is_visible()
                            is_enabled = await element.is_enabled()
                            if is_visible and is_enabled:
                                pagination_element = element
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞ '–°–ª–µ–¥—É—é—â–∞—è': {selector}")
                                break
                    except:
                        continue
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —ç–ª–µ–º–µ–Ω—Ç - –∫–ª–∏–∫–∞–µ–º
            if pagination_element:
                try:
                    # –°–∫—Ä–æ–ª–ª–∏–º –∫ —ç–ª–µ–º–µ–Ω—Ç—É
                    await pagination_element.scroll_into_view_if_needed()
                    await self.random_delay(1, 2)
                    
                    # –ö–ª–∏–∫–∞–µ–º
                    await pagination_element.click()
                    logger.info(f"üéØ –ö–ª–∏–∫–Ω—É–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}")
                    
                    # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
                    await self.random_delay(4, 6)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å
                    await self.page.wait_for_load_state('networkidle', timeout=10000)
                    
                    return True
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∏–∫–µ: {e}")
                    return False
            else:
                # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—â–µ–º –í–°–ï —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                logger.info("üîç –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏...")
                
                try:
                    # –ò—â–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                    all_pagination = await self.page.query_selector_all('a, button')
                    
                    for element in all_pagination:
                        try:
                            text = await element.text_content()
                            if text and text.strip() == str(page_number):
                                is_visible = await element.is_visible()
                                is_enabled = await element.is_enabled()
                                
                                if is_visible and is_enabled:
                                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –≤ –æ–±—â–µ–º –ø–æ–∏—Å–∫–µ!")
                                    await element.scroll_into_view_if_needed()
                                    await self.random_delay(1, 2)
                                    await element.click()
                                    await self.random_delay(4, 6)
                                    await self.page.wait_for_load_state('networkidle', timeout=10000)
                                    return True
                        except:
                            continue
                            
                except:
                    pass
                
                logger.info(f"‚ùå –ö–Ω–æ–ø–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
                
        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {e}")
            return False
            
    async def extract_business_info(self, url: str, category: str) -> Optional[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            logger.info(f"–ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            await self.page.goto(url, wait_until="domcontentloaded", timeout=20000)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            await self.wait_for_dynamic_content()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            name = None
            address = None
            phone = None
            website = None
            whatsapp = None
            instagram = None
            
            try:
                name = await self.extract_text_by_selectors([
                    'h1', 'h2', '[class*="title"]', '[class*="name"]', '[class*="header"]'
                ])
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏—è: {e}")
            
            try:
                address = await self.extract_address()
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞–¥—Ä–µ—Å–∞: {e}")
            
            try:
                phone = await self.extract_phone()
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–∞: {e}")
            
            try:
                website = await self.extract_website()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å–∞–π—Ç–∞: {e}")
            
            try:
                whatsapp = await self.extract_whatsapp()
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ WhatsApp: {e}")
            
            try:
                instagram = await self.extract_instagram()
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ Instagram: {e}")
            
            result = {
                '–ù–∞–∑–≤–∞–Ω–∏–µ': name or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
                '–ê–¥—Ä–µ—Å': address or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
                '–¢–µ–ª–µ—Ñ–æ–Ω': phone or '–ù–µ —É–∫–∞–∑–∞–Ω–æ', 
                '–°–∞–π—Ç': website or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
                'WhatsApp': whatsapp or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
                'Instagram': instagram or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
                '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': category,
                '–ï—Å—Ç—å —Å–∞–π—Ç': '–î–∞' if website and website != '–ù–µ —É–∫–∞–∑–∞–Ω–æ' else '–ù–µ—Ç'
            }
            
            logger.info(f"–°–æ–±—Ä–∞–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {result['–ù–∞–∑–≤–∞–Ω–∏–µ']}, {result['–ê–¥—Ä–µ—Å']}, {result['–¢–µ–ª–µ—Ñ–æ–Ω']}")
            if website and website != '–ù–µ —É–∫–∞–∑–∞–Ω–æ':
                logger.info(f"–°–∞–π—Ç: {result['–°–∞–π—Ç']}")
            if whatsapp and whatsapp != '–ù–µ —É–∫–∞–∑–∞–Ω–æ':
                logger.info(f"WhatsApp: {result['WhatsApp']}")
            if instagram and instagram != '–ù–µ —É–∫–∞–∑–∞–Ω–æ':
                logger.info(f"Instagram: {result['Instagram']}")
            
            return result
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å {url}: {e}")
            return None
            
    async def extract_text_by_selectors(self, selectors: List[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å–ø–∏—Å–∫—É —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤"""
        for selector in selectors:
            try:
                element = await self.page.query_selector(selector)
                if element:
                    text = await element.text_content()
                    if text and text.strip():
                        return text.strip()
            except:
                continue
        return None
        
    async def extract_address(self) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        address_selectors = [
            '[class*="address"]',
            '[class*="location"]', 
            '.address',
            '.location'
        ]
        
        address = await self.extract_text_by_selectors(address_selectors)
        if address:
            return address
            
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        try:
            page_text = await self.page.text_content('body')
            if page_text:
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–¥—Ä–µ—Å–∞
                address_patterns = [
                    r'–ñ–ö\s+[–ê-–Ø–∞-—è\s]+,\s*—É–ª–∏—Ü–∞\s+[–ê-–Ø–∞-—è\s]+,\s*\d+',
                    r'—É–ª–∏—Ü–∞\s+[–ê-–Ø–∞-—è\s]+,\s*\d+[–ê-–Ø–∞-—è\s]*',
                    r'–ø—Ä–æ—Å–ø–µ–∫—Ç\s+[–ê-–Ø–∞-—è\s]+,\s*\d+[–ê-–Ø–∞-—è\s]*',
                    r'–±—É–ª—å–≤–∞—Ä\s+[–ê-–Ø–∞-—è\s]+,\s*\d+[–ê-–Ø–∞-—è\s]*',
                    r'[–ê-–Ø–∞-—è\s]+(—Ä–∞–π–æ–Ω|–º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω)[–ê-–Ø–∞-—è\s\d,]*',
                    r'–ê—Å—Ç–∞–Ω–∞[,\s]+[–ê-–Ø–∞-—è\s\d,]+'
                ]
                
                for pattern in address_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        return match.group().strip()
        except:
            pass
            
        return None
        
    async def extract_phone(self) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        # –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ –∏ —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏
        phone_selectors = [
            'a[href^="tel:"]',
            '[class*="phone"]',
            'button[class*="phone"]'
        ]
        
        for selector in phone_selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                for element in elements:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º href
                    href = await element.get_attribute('href')
                    if href and href.startswith('tel:'):
                        return href.replace('tel:', '').strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç
                    text = await element.text_content()
                    if text and re.search(r'[\d\-\+\(\)\s]{7,}', text):
                        return text.strip()
            except:
                continue
                
        # –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        try:
            page_text = await self.page.text_content('body')
            if page_text:
                phone_patterns = [
                    r'\+7\s*[-\(\s]*\d{3}\s*[-\)\s]*\d{3}[-\s]*\d{2}[-\s]*\d{2}',
                    r'8\s*[-\(\s]*\d{3}\s*[-\)\s]*\d{3}[-\s]*\d{2}[-\s]*\d{2}',
                    r'\+7\d{10}',
                    r'8\d{10}'
                ]
                
                for pattern in phone_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        return match.group().strip()
        except:
            pass
            
        return None

    async def wait_for_dynamic_content(self):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            logger.debug("–ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å–µ—Ç–∏
            await self.page.wait_for_load_state('networkidle', timeout=15000)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è JavaScript
            await asyncio.sleep(5)
            
            # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            key_selectors = [
                'h1, h2',  # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                '[class*="contact"]',  # –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                '[class*="phone"]',   # –¢–µ–ª–µ—Ñ–æ–Ω
                'button, a'  # –ö–Ω–æ–ø–∫–∏ –∏ —Å—Å—ã–ª–∫–∏
            ]
            
            for selector in key_selectors:
                try:
                    await self.page.wait_for_selector(selector, timeout=3000)
                    logger.debug(f"–ù–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç: {selector}")
                    break
                except:
                    continue
            
            # –ï—â–µ –Ω–µ–º–Ω–æ–≥–æ –ø–æ–¥–æ–∂–¥–µ–º –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            await asyncio.sleep(2)
            
            logger.debug("–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")
            
        except Exception as e:
            logger.debug(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–∂–∏–¥–∞–Ω–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")

    async def decode_2gis_website_link(self, link: str) -> Optional[str]:
        """–ü—Ä–æ—Å—Ç–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ 2–ì–ò–° —Å—Å—ã–ª–æ–∫"""
        try:
            import base64
            
            if 'link.2gis.com' not in link:
                return None
                
            logger.debug(f"–î–µ–∫–æ–¥–∏—Ä—É–µ–º 2–ì–ò–° —Å—Å—ã–ª–∫—É: {link}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º base64 —á–∞—Å—Ç—å
            parts = link.split('/')
            if len(parts) < 2:
                return None
                
            encoded_part = parts[-1]
            
            # –£–¥–∞–ª—è–µ–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if '?' in encoded_part:
                encoded_part = encoded_part.split('?')[0]
            if '#' in encoded_part:
                encoded_part = encoded_part.split('#')[0]
                
            try:
                # –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ padding
                for padding in ['', '=', '==', '===']:
                    try:
                        padded_data = encoded_part + padding
                        decoded_bytes = base64.b64decode(padded_data)
                        decoded_string = decoded_bytes.decode('utf-8')
                        
                        # –ò—â–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã–µ –¥–æ–º–µ–Ω—ã
                        domain_patterns = [
                            r'\b([a-zA-Z0-9][-a-zA-Z0-9]*\.(?:kz|com|ru|org|net))\b'
                        ]
                        
                        for pattern in domain_patterns:
                            matches = re.findall(pattern, decoded_string, re.IGNORECASE)
                            for domain in matches:
                                # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –¥–æ–º–µ–Ω—ã
                                if not any(bad in domain.lower() for bad in ['2gis', 'sberbank', 'yandex', 'google']):
                                    if len(domain) > 6:
                                        result = f"https://{domain}"
                                        logger.info(f"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω —Å–∞–π—Ç: {result}")
                                        return result
                        
                        break
                    except:
                        continue
                        
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                
            return None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Å—ã–ª–∫–∏: {e}")
            return None

    async def extract_website(self) -> Optional[str]:
        """–ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∞–π—Ç–∞ —Ç–æ–ª—å–∫–æ –ø–æ –∫–ª–∞—Å—Å—É _49kxlr"""
        try:
            logger.info("–ò—â–µ–º —Å–∞–π—Ç —Ç–æ–ª—å–∫–æ –≤ —ç–ª–µ–º–µ–Ω—Ç–∞—Ö —Å –∫–ª–∞—Å—Å–æ–º _49kxlr...")

            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–æ–º _49kxlr
            try:
                elements = await self.page.query_selector_all('._49kxlr')
                logger.debug(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –∫–ª–∞—Å—Å–æ–º _49kxlr: {len(elements)}")
                
                for element in elements:
                    try:
                        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
                        links = await element.query_selector_all('a')
                        for link in links:
                            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏
                            link_text = await link.text_content()
                            if link_text:
                                link_text = link_text.strip()
                                logger.debug(f"–ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏: {link_text}")
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–æ–º–µ–Ω
                                if re.match(r'^[a-zA-Z0-9][-a-zA-Z0-9]*\.(?:kz|com|ru|org|net|biz|cafe|coffee)$', link_text):
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –¥–æ–º–µ–Ω–∞
                                    if len(link_text) > 6:
                                        result = f"https://{link_text}"
                                        logger.info(f"–ù–∞–π–¥–µ–Ω —Å–∞–π—Ç –≤ _49kxlr: {result}")
                                        return result
                            
                            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º href –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —Ç–∞–º –ø—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞
                            href = await link.get_attribute('href')
                            if href and 'link.2gis.com' in href:
                                # –ü–æ–ø—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å 2–ì–ò–° —Å—Å—ã–ª–∫—É
                                decoded_site = await self.decode_2gis_website_link(href)
                                if decoded_site:
                                    logger.info(f"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω —Å–∞–π—Ç –∏–∑ _49kxlr: {decoded_site}")
                                    return decoded_site
                                    
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–ª–µ–º–µ–Ω—Ç–∞ _49kxlr: {e}")
                        continue
                        
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ _49kxlr: {e}")

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ _49kxlr, –ø—Ä–æ–±—É–µ–º –ø–æ—Ö–æ–∂–∏–µ –∫–ª–∞—Å—Å—ã
            try:
                similar_selectors = [
                    '[class*="49kxlr"]',
                    '[class*="kxlr"]',
                    '[class*="_rehek"]',  # –ò–∑ –≤–∞—à–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –±—ã–ª –∫–ª–∞—Å—Å _1rehek
                    '[class*="rehek"]'
                ]
                
                for selector in similar_selectors:
                    try:
                        elements = await self.page.query_selector_all(selector)
                        logger.debug(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º {selector}: {len(elements)}")
                        
                        for element in elements:
                            links = await element.query_selector_all('a')
                            for link in links:
                                link_text = await link.text_content()
                                if link_text:
                                    link_text = link_text.strip()
                                    
                                    # –°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–æ–º–µ–Ω
                                    if re.match(r'^[a-zA-Z0-9][-a-zA-Z0-9]*\.(?:kz|com|ru|org|net)$', link_text):
                                        if len(link_text) > 6 and '2gis' not in link_text.lower():
                                            result = f"https://{link_text}"
                                            logger.info(f"–ù–∞–π–¥–µ–Ω —Å–∞–π—Ç –≤ {selector}: {result}")
                                            return result
                                            
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ {selector}: {e}")
                        continue
                        
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ—Ö–æ–∂–∏—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤: {e}")

            logger.info("–°–∞–π—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —ç–ª–µ–º–µ–Ω—Ç–∞—Ö _49kxlr")
            return None
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–∞–π—Ç–∞: {e}")
            return None
        
    async def decode_2gis_link(self, link: str) -> Optional[str]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Å—ã–ª–æ–∫ 2–ì–ò–°"""
        try:
            import base64
            import urllib.parse
            
            if 'link.2gis.com' not in link:
                return None
                
            logger.debug(f"–ü—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å: {link}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º base64 —á–∞—Å—Ç—å –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ /
            parts = link.split('/')
            if len(parts) < 2:
                return None
                
            encoded_part = parts[-1]
            
            # –£–¥–∞–ª—è–µ–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å
            if '?' in encoded_part:
                encoded_part = encoded_part.split('?')[0]
            if '#' in encoded_part:
                encoded_part = encoded_part.split('#')[0]
                
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
                for padding in ['', '=', '==', '===']:
                    try:
                        padded_data = encoded_part + padding
                        decoded_bytes = base64.b64decode(padded_data)
                        decoded_string = decoded_bytes.decode('utf-8')
                        
                        # –ò—â–µ–º wa.me —Å—Å—ã–ª–∫—É –≤ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
                        wa_patterns = [
                            r'(https://wa\.me/[^\s%"\']+)',
                            r'wa\.me/(\d+)',
                            r'whatsapp://send\?phone=(\d+)'
                        ]
                        
                        for pattern in wa_patterns:
                            matches = re.findall(pattern, decoded_string)
                            for match in matches:
                                if match.startswith('https://'):
                                    wa_url = urllib.parse.unquote(match)
                                    logger.info(f"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp: {wa_url}")
                                    return wa_url
                                elif match.isdigit():
                                    wa_url = f"https://wa.me/{match}"
                                    logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp: {wa_url}")
                                    return wa_url
                        
                        break  # –ï—Å–ª–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ, –≤—ã—Ö–æ–¥–∏–º
                    except:
                        continue
                        
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è base64: {e}")
                
            return None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Å—ã–ª–∫–∏ 2gis: {e}")
            return None

    async def extract_whatsapp(self) -> Optional[str]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ WhatsApp"""
        try:
            # 1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ WhatsApp
            direct_selectors = [
                'a[href*="wa.me"]',
                'a[href*="whatsapp"]',
                'a[href*="link.2gis.com"]'
            ]
            
            for selector in direct_selectors:
                try:
                    links = await self.page.query_selector_all(selector)
                    for link in links:
                        href = await link.get_attribute('href')
                        if href:
                            if 'wa.me' in href or 'whatsapp' in href:
                                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –ø—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ WhatsApp: {href}")
                                return href
                            elif 'link.2gis.com' in href:
                                # –ü—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Å—Å—ã–ª–∫—É 2gis
                                decoded = await self.decode_2gis_link(href)
                                if decoded:
                                    return decoded
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ {selector}: {e}")
                    continue

            # 2. –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ WhatsApp –ë–ï–ó –ö–õ–ò–ö–ê - —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã
            whatsapp_selectors = [
                'button[title*="WhatsApp"], button[title*="whatsapp"]',
                'a[title*="WhatsApp"], a[title*="whatsapp"]',
                'div[title*="WhatsApp"], div[title*="whatsapp"]',
                '[class*="whatsapp"]',
                '[data-social="whatsapp"]',
                'button:has-text("WhatsApp")',
                'a:has-text("WhatsApp")'
            ]
            
            for selector in whatsapp_selectors:
                try:
                    buttons = await self.page.query_selector_all(selector)
                    for button in buttons:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –ë–ï–ó –ö–õ–ò–ö–ê
                        attributes_to_check = [
                            'href', 'data-url', 'data-link', 'data-phone', 
                            'data-action', 'data-contact', 'onclick',
                            'data-whatsapp', 'data-phone-number'
                        ]
                        
                        for attr in attributes_to_check:
                            try:
                                attr_value = await button.get_attribute(attr)
                                if not attr_value:
                                    continue
                                    
                                # –ò—â–µ–º –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –≤ –∞—Ç—Ä–∏–±—É—Ç–µ
                                phone_patterns = [
                                    r'(\+?7\d{10})',
                                    r'wa\.me/(\d+)',
                                    r'whatsapp://send\?phone=(\d+)'
                                ]
                                
                                for pattern in phone_patterns:
                                    match = re.search(pattern, attr_value)
                                    if match:
                                        phone = match.group(1).replace('+', '')
                                        if not phone.startswith('7') and len(phone) == 10:
                                            phone = '7' + phone
                                        elif not phone.startswith('7') and len(phone) == 11:
                                            phone = phone
                                        result = f"https://wa.me/{phone}"
                                        logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp –∏–∑ {attr}: {result}")
                                        return result
                                
                                # –ò—â–µ–º –≥–æ—Ç–æ–≤—É—é —Å—Å—ã–ª–∫—É WhatsApp
                                if 'wa.me' in attr_value or 'whatsapp' in attr_value:
                                    wa_match = re.search(r'(https://wa\.me/[^\s\'"]+)', attr_value)
                                    if wa_match:
                                        logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp –≤ {attr}: {wa_match.group(1)}")
                                        return wa_match.group(1)
                            except Exception as e:
                                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∞—Ç—Ä–∏–±—É—Ç–∞ {attr}: {e}")
                                continue
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ {selector}: {e}")
                    continue

            # 3. –ò—â–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            try:
                page_content = await self.page.content()
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ 2gis —Å –≤–æ–∑–º–æ–∂–Ω—ã–º WhatsApp
                gis_patterns = [
                    r'href=["\']([^"\']*link\.2gis\.com[^"\']*)["\']',
                    r'(https://link\.2gis\.com/[^\s\'"]+)'
                ]
                
                for pattern in gis_patterns:
                    matches = re.findall(pattern, page_content)
                    for match in matches:
                        decoded = await self.decode_2gis_link(match)
                        if decoded:
                            return decoded
                
                # –ò—â–µ–º –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ WhatsApp
                whatsapp_patterns = [
                    r'whatsapp[^0-9]*(\+?7\d{10})',
                    r'wa\.me/(\d+)',
                    r'data-phone["\']:\s*["\'](\+?7\d{10})["\']',
                    r'phone["\']:\s*["\'](\+?7\d{10})["\']',
                    r'whatsapp["\']?\s*:\s*["\']([^"\']+)["\']'
                ]
                
                for pattern in whatsapp_patterns:
                    matches = re.findall(pattern, page_content, re.IGNORECASE)
                    for match in matches:
                        if 'wa.me' in match or 'whatsapp' in match:
                            logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp –≤ –∫–æ–¥–µ: {match}")
                            return match
                        elif re.match(r'\+?7?\d{10,11}', match):
                            phone = match.replace('+', '').replace(' ', '').replace('-', '')
                            if len(phone) == 11 and phone.startswith('7'):
                                result = f"https://wa.me/{phone}"
                            elif len(phone) == 10:
                                result = f"https://wa.me/7{phone}"
                            else:
                                continue
                            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞: {result}")
                            return result
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ: {e}")

            # 4. –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—â–µ–º –ª—é–±—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –Ω–æ–º–µ—Ä–æ–≤ —Ä—è–¥–æ–º —Å WhatsApp –≤ —Ç–µ–∫—Å—Ç–µ
            try:
                page_text = await self.page.text_content('body')
                if page_text:
                    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º WhatsApp
                    text_lower = page_text.lower()
                    whatsapp_pos = text_lower.find('whatsapp')
                    
                    if whatsapp_pos != -1:
                        # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –≤ —Ä–∞–¥–∏—É—Å–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç —Å–ª–æ–≤–∞ WhatsApp
                        start = max(0, whatsapp_pos - 100)
                        end = min(len(page_text), whatsapp_pos + 100)
                        context = page_text[start:end]
                        
                        # –ò—â–µ–º –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –≤ —ç—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
                        phone_match = re.search(r'(\+?7\d{10})', context)
                        if phone_match:
                            phone = phone_match.group(1).replace('+', '')
                            if not phone.startswith('7'):
                                phone = '7' + phone
                            result = f"https://wa.me/{phone}"
                            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ WhatsApp –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {result}")
                            return result
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ —Ç–µ–∫—Å—Ç–µ: {e}")
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ WhatsApp: {e}")
            
        return None
        
    async def extract_instagram(self) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Instagram - —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ Instagram
            instagram_links = await self.page.query_selector_all('a[href*="instagram"]')
            for link in instagram_links:
                href = await link.get_attribute('href')
                if href and 'instagram' in href:
                    logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ Instagram: {href}")
                    return href
                    
            # –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ Instagram –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–±—ã—Ç–∏—è
            instagram_buttons = await self.page.query_selector_all('button, div, span, a')
            for button in instagram_buttons:
                try:
                    text = await button.text_content()
                    if text and 'instagram' in text.lower():
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º onclick
                        onclick = await button.get_attribute('onclick')
                        if onclick:
                            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤ onclick
                            instagram_match = re.search(r'(https://[^\'"\s]*instagram\.com[^\'"\s]*)', onclick)
                            if instagram_match:
                                link = instagram_match.group(1)
                                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ Instagram –≤ onclick: {link}")
                                return link
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º data-–∞—Ç—Ä–∏–±—É—Ç—ã
                        for attr in ['data-url', 'data-link', 'data-href', 'data-instagram', 'data-action']:
                            attr_value = await button.get_attribute(attr)
                            if attr_value and 'instagram' in attr_value:
                                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ Instagram –≤ {attr}: {attr_value}")
                                return attr_value
                        
                        # –ò—â–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Å—Å—ã–ª–∫–∞–º–∏
                        parent = button
                        for _ in range(3):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 3 —É—Ä–æ–≤–Ω–µ–π –≤–≤–µ—Ä—Ö
                            try:
                                parent = await parent.query_selector('xpath=..')
                                if parent:
                                    parent_href = await parent.get_attribute('href')
                                    if parent_href and 'instagram' in parent_href:
                                        logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ Instagram –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ: {parent_href}")
                                        return parent_href
                            except:
                                break
                except:
                    continue
                    
            # –ò—â–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            try:
                page_content = await self.page.content()
                # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å Instagram —Å—Å—ã–ª–∫–∞–º–∏
                js_patterns = [
                    r'instagram["\']?\s*:\s*["\']([^"\']+)["\']',
                    r'instagram\.com/([^"\')\s/]+)',
                    r'https://(?:www\.)?instagram\.com/([^"\')\s/]+)',
                    r'window\.open\(["\']([^"\']*instagram[^"\']*)["\']',
                    r'href\s*=\s*["\']([^"\']*instagram[^"\']*)["\']'
                ]
                
                for pattern in js_patterns:
                    matches = re.findall(pattern, page_content, re.IGNORECASE)
                    for match in matches:
                        if match and 'instagram' not in match:
                            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω —Ç–æ–ª—å–∫–æ username, —Å–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É
                            match = f"https://instagram.com/{match}"
                        elif match and 'instagram' in match and not match.startswith('http'):
                            match = f"https://{match}"
                        
                        if match and 'instagram.com' in match:
                            logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ Instagram –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ: {match}")
                            return match
            except:
                pass
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ Instagram: {e}")
            
        return None
        
    async def save_to_excel(self, filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Excel"""
        try:
            if not self.results:
                logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                return
                
            df = pd.DataFrame(self.results)
            
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏', index=False)
                
                worksheet = writer.sheets['–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏']
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
                    
            logger.info(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filename}")
            logger.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(self.results)}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Excel: {e}")
            
    async def parse_category(self, category: str):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            if not await self.open_2gis_and_search(category):
                return
                
            # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤—Å–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
            business_urls = await self.get_business_links_pagination_fixed()
            
            if not business_urls:
                logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}'")
                return
                
            logger.info(f"–ë—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å {len(business_urls)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
            processed = 0
            for i, url in enumerate(business_urls, 1):
                try:
                    logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é {i}/{len(business_urls)}")
                    
                    business_info = await self.extract_business_info(url, category)
                    if business_info:
                        self.results.append(business_info)
                        processed += 1
                        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ: {business_info['–ù–∞–∑–≤–∞–Ω–∏–µ']}")
                    
                    await self.random_delay(2, 4)
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ {i}: {e}")
                    continue
                    
            logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}'. –°–æ–±—Ä–∞–Ω–æ {processed} –∑–∞–ø–∏—Å–µ–π")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}': {e}")
            
    async def run(self, categories: List[str]):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
        try:
            await self.setup_browser()
            
            for category in categories:
                await self.parse_category(category)
                await self.random_delay(3, 5)
                
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"2gis_results_{self.city}_{timestamp}.xlsx"
            await self.save_to_excel(filename)
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            raise
            
        finally:
            if self.browser:
                try:
                    await self.browser.close()
                    logger.info("–ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")
                except:
                    pass
            if hasattr(self, 'playwright'):
                try:
                    await self.playwright.stop()
                except:
                    pass

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä 2–ì–ò–° —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π')
    parser.add_argument('--city', '-c', default='–ê—Å—Ç–∞–Ω–∞', help='–ì–æ—Ä–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞')
    parser.add_argument('--categories', '-cat', nargs='+', 
                       default=['–∫–æ—Ñ–µ–π–Ω–∏'],
                       help='–°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
    parser.add_argument('--max-items', '-m', type=int, default=100,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é')
    parser.add_argument('--config', help='–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π')
    
    args = parser.parse_args()
    
    if args.config:
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                config = json.load(f)
                args.city = config.get('city', args.city)
                args.categories = config.get('categories', args.categories)
                args.max_items = config.get('max_items', args.max_items)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    
    parser_instance = GISParser(city=args.city, max_items_per_category=args.max_items)
    
    try:
        asyncio.run(parser_instance.run(args.categories))
        logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    except Exception as e:
        logger.error(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —Å –æ—à–∏–±–∫–æ–π: {e}")

if __name__ == "__main__":
    main()